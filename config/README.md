# Training Configuration Files

This directory contains training configurations for fine-tuning the Chatterbox TTS model on Amharic data.

## ğŸ“‹ Available Configurations

### 1. `training_config_finetune_FIXED.yaml` â­ **RECOMMENDED**
**Use this for production finetuning!**

This is the **FIXED** configuration that properly preserves multilingual capabilities while adding Amharic.

**Key Features:**
- âœ… **Freezes original embeddings** (tokens 0-2453)
- âœ… **Low learning rate** (1e-5) to prevent weight destruction
- âœ… **Proper gradient clipping** (0.5) for stability
- âœ… **Conservative optimizer settings**
- âœ… **Validates against all languages** (Amharic + English + French + German)

**When to use:**
- Starting fresh with extended model
- Fixing broken finetuning that destroyed pretrained languages
- Production deployments
- When you need reliable multilingual TTS

**Expected Results:**
- ğŸ¯ Amharic: Clear speech after training
- âœ… English: Still works (preserved)
- âœ… French: Still works (preserved)
- âœ… German: Still works (preserved)

---

### 2. `training_config.yaml`
**Standard configuration** - Now enhanced with safer finetuning settings.

**Recent improvements:**
- Learning rate reduced to `1e-5` (from `2e-4`)
- Beta values changed to `[0.9, 0.999]` (more conservative)
- Gradient clipping tightened to `0.5` (from `1.0`)
- Early stopping patience increased to `50`

**When to use:**
- General training
- When you've already validated your setup
- Intermediate experiments

---

### 3. `training_config_stable.yaml`
**Optimized for small datasets** (<1000 samples)

**Special Features:**
- Smaller batch size (8)
- More frequent checkpoints (every 500 steps)
- More frequent evaluation (every 100 steps)
- Higher dropout (0.2) to prevent overfitting
- Lower max epochs (200) to prevent overfitting

**When to use:**
- Training on small Amharic datasets (<1 hour of audio)
- Limited compute resources
- Quick experiments
- When you see overfitting signs

---

## ğŸ¯ Which Config Should I Use?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DECISION TREE                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Q: Is your English/French audio noisy after training?     â”‚
â”‚     â”œâ”€ YES â†’ Use training_config_finetune_FIXED.yaml  âœ…   â”‚
â”‚     â””â”€ NO  â†’ Continue...                                   â”‚
â”‚                                                             â”‚
â”‚  Q: Is this your first time training?                      â”‚
â”‚     â”œâ”€ YES â†’ Use training_config_finetune_FIXED.yaml  âœ…   â”‚
â”‚     â””â”€ NO  â†’ Continue...                                   â”‚
â”‚                                                             â”‚
â”‚  Q: Do you have <1000 audio samples?                       â”‚
â”‚     â”œâ”€ YES â†’ Use training_config_stable.yaml           âœ…   â”‚
â”‚     â””â”€ NO  â†’ Use training_config.yaml                  âœ…   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš¨ Critical Settings Explained

### **1. Embedding Freezing** ğŸ”’
```yaml
freeze_original_embeddings: true
freeze_until_index: 2454
```

**What it does:**
- Freezes embeddings for indices 0-2453 (English, French, etc.)
- Allows training only embeddings 2454+ (Amharic)

**Why it matters:**
- âœ… Preserves pretrained knowledge
- âœ… Prevents catastrophic forgetting
- âœ… Keeps all 23 languages working

**Warning:** If you set this to `false`, you WILL destroy pretrained languages! ğŸ’€

---

### **2. Learning Rate** ğŸ“‰
```yaml
learning_rate: 1.0e-5  # FIXED config
learning_rate: 5.0e-5  # STABLE config
```

**Comparison:**
```
Pretraining LR:  2e-4  â†’ Train from scratch
Finetuning LR:   1e-5  â†’ Preserve & adapt (FIXED)
Small data LR:   5e-5  â†’ Fast adaptation (STABLE)
```

**Why low LR?**
- High LR = Big weight updates = Destroys pretrained knowledge âŒ
- Low LR = Small updates = Preserves knowledge + learns new âœ…

---

### **3. Vocabulary Sizes** ğŸ“š
```yaml
original_vocab_size: 2454  # Chatterbox (23 languages)
n_vocab: 2535              # Extended (23 langs + Amharic)
```

**Math:**
```
2454 (Chatterbox original)
+  81 (Amharic tokens)
â”€â”€â”€â”€â”€
2535 (Total)
```

**Important:** Your merged tokenizer MUST have exactly this size!

---

### **4. Gradient Clipping** âœ‚ï¸
```yaml
grad_clip_thresh: 0.5
```

**What it prevents:**
- Exploding gradients
- NaN losses
- Weight corruption
- Training instability

**Rule:** Lower = more stable, but slower learning

---

## ğŸ“Š Expected Training Behavior

### **Normal Training (with FIXED config):**
```
Epoch 1-10:    Loss ~10 â†’ Basic patterns
Epoch 10-50:   Loss ~5  â†’ Words forming
Epoch 50-100:  Loss ~3  â†’ Clear words
Epoch 100-200: Loss ~2  â†’ Natural speech
Epoch 200+:    Loss ~1  â†’ High quality
```

**Checkpoints to test:**
- Test Amharic at epoch 50
- Test English/French at epoch 50 (should still work!)
- Continue if both work

---

### **Warning Signs:**

âŒ **Loss increases:**
- Learning rate too high
- Bad data in batch
- Gradient explosion

âŒ **Loss stays flat:**
- Learning rate too low
- Embeddings actually frozen (not just first 2454)
- Bad initialization

âŒ **Loss decreases but audio is noise:**
- Wrong audio preprocessing
- Mel-spectrogram issues
- Vocoder problems

---

## ğŸ”§ How to Use These Configs

### **Option 1: Gradio UI (Recommended)**
```python
# In Gradio app:
1. Go to "Training Pipeline" tab
2. Select config: training_config_finetune_FIXED.yaml
3. Verify settings:
   âœ… Freeze Original Embeddings: CHECKED
   âœ… Freeze Until Index: 2454
   âœ… Learning Rate: 0.00001
4. Click "Start Training"
```

---

### **Option 2: Command Line**
```bash
# Production finetuning (FIXED config)
python src/training/train.py \
  --config config/training_config_finetune_FIXED.yaml \
  --data data/srt_datasets/my_dataset

# Small dataset (STABLE config)
python src/training/train.py \
  --config config/training_config_stable.yaml \
  --data data/srt_datasets/my_small_dataset

# Standard training
python src/training/train.py \
  --config config/training_config.yaml \
  --data data/srt_datasets/my_dataset
```

---

## ğŸ“ Understanding the Settings

### **Batch Size & Gradient Accumulation**
```yaml
batch_size: 16
grad_accumulation_steps: 2
# Effective batch size = 16 Ã— 2 = 32
```

**Why gradient accumulation?**
- Larger effective batch size without OOM
- More stable gradients
- Better training dynamics

---

### **Early Stopping**
```yaml
early_stopping: true
patience: 50
min_epochs: 30
min_delta: 0.001
```

**How it works:**
```
If validation loss doesn't improve by 0.001
for 50 consecutive evaluations (after epoch 30)
â†’ Stop training
â†’ Restore best checkpoint
```

---

## ğŸ“ˆ Monitoring Training

### **What to watch:**
1. **Training loss** should decrease steadily
2. **Validation loss** should decrease (but slower)
3. **Learning rate** should decay gradually
4. **Generated audio** should improve over time

### **TensorBoard:**
```bash
tensorboard --logdir logs
# Open: http://localhost:6006
```

---

## ğŸ†˜ Troubleshooting

### **Problem: All languages sound like noise**
**Cause:** `freeze_original_embeddings: false` or LR too high

**Fix:**
1. Use `training_config_finetune_FIXED.yaml`
2. Start from fresh extended model (not corrupted checkpoint)
3. Verify `freeze_original_embeddings: true`

---

### **Problem: Loss doesn't decrease**
**Possible causes:**
- Learning rate too low â†’ Try 2e-5
- Dataset issues â†’ Verify audio files
- Model not loaded â†’ Check pretrained path

---

### **Problem: Training too slow**
**Solutions:**
- Increase batch size (if GPU allows)
- Increase learning rate slightly (max 2e-5)
- Use fewer evaluation steps

---

### **Problem: Overfitting (train loss low, val loss high)**
**Solutions:**
- Use `training_config_stable.yaml`
- Increase dropout
- Get more data
- Enable early stopping

---

## ğŸ“š Additional Resources

- **[Fixing Broken Finetuning Guide](../docs/FIXING_BROKEN_FINETUNING.md)** - Comprehensive recovery guide
- **[Training Documentation](../docs/TRAINING.md)** - Detailed training guide
- **[Dataset Preparation](../docs/DATASET_PREPARATION.md)** - How to prepare your data

---

## âœ… Pre-Flight Checklist

Before starting training, verify:

- [ ] Extended model exists at `models/pretrained/chatterbox_extended.pt`
- [ ] Merged tokenizer exists at `models/tokenizer/Am_tokenizer_merged.json`
- [ ] Tokenizer vocab size is 2535
- [ ] Dataset has train/val splits (`metadata_train.csv`, `metadata_val.csv`)
- [ ] Config has `freeze_original_embeddings: true`
- [ ] Config has `freeze_until_index: 2454`
- [ ] Config has `learning_rate: 1e-5` (for production)
- [ ] Audio files are in `data/.../wavs/` directory
- [ ] GPU is available (check with `nvidia-smi`)

---

## ğŸ‰ Success Criteria

Your finetuning is successful when:

âœ… **Amharic works:**
```
Input:  "áˆ°áˆ‹áˆ áˆˆá‹“áˆˆáˆ"
Output: Clear Amharic speech ğŸ™ï¸
```

âœ… **English still works:**
```
Input:  "Hello world"
Output: Clear English speech ğŸ™ï¸
```

âœ… **French still works:**
```
Input:  "Bonjour le monde"
Output: Clear French speech ğŸ™ï¸
```

ğŸŠ **Congratulations! You've successfully added Amharic to a multilingual TTS model!**

---

**Last Updated:** 2025-01-04  
**Recommended Config:** `training_config_finetune_FIXED.yaml`
